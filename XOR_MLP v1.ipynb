{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22/11/2019\n",
    "\n",
    "The code here is close to Nielsen. Each activation is treated as a column vector, even the last one which for XOR is just a simple number and is encloded in a shape (1,1) column vector of just one row, i.e if activation value of output neuron is a, then it is computed as np.array([[a]]).\n",
    "\n",
    "Can easily adapt code here for the MLP excercises and the Iris classification problem.\n",
    "But you may need to use more than 2 hidden neurons and more than 1 output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigm(z):\n",
    "    return  1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigm_deriv(z):\n",
    "    a = sigm(z)\n",
    "    return a*(1 - a)\n",
    "\n",
    "# Cross-Entropy Cost function\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-15  # To avoid log(0) error\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip values to avoid log(0)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XOR_MLP:\n",
    "    def __init__(self):\n",
    "        self.train_inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "        self.train_outputs = np.array([0,1,1,0])\n",
    "          \n",
    "        np.random.seed(23)\n",
    "        # hidden layer of 2 neurons\n",
    "        self.w2 = np.random.randn(2,2)\n",
    "        self.b2 = np.random.randn(2,1)\n",
    "        \n",
    "        # output layer has 1 neuron\n",
    "        self.w3 = np.random.randn(1,2)\n",
    "        self.b3 = np.random.randn(1,1)\n",
    "        \n",
    "\n",
    "    def feedforward(self, xs):    \n",
    "        # here xs is a matrix where each column is an input vector\n",
    "        # w2.dot(xs) applies the weight matrix w2 to each input at once\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)            \n",
    "        return a3s\n",
    "\n",
    "    \n",
    "    def backprop(self, xs, ys):   # Assumed here that input vectors are rows in xs\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        \n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x,y in zip(xs,ys):               # for zip to work, each x in xs must be a row vector\n",
    "            a1 = x.reshape(2,1)              # convert input row vector x into (2,1) column vector\n",
    "            z2 = self.w2.dot(a1) + self.b2   # so will z2 and a2\n",
    "            a2 = sigm(z2)                    # column vector shape (2,1)\n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3   # a simple number in a (1,1) column vector\n",
    "            a3 = sigm(z3)                    # so is a3\n",
    "            \n",
    "            delta3 = (a3-y) * sigm_deriv(z3)                   # delta3.shape is (1,1)\n",
    "           \n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))  # w3 shape is (1,2), w3.T shape is (2,1)\n",
    "                                                               # delta2 is shape (2,1)\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)  # shape (1,1) by (1,2) gives (1,2)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)  # shape (2,1) by (1,2) gives (2,2)\n",
    "        \n",
    "           \n",
    "            cost += ((a3 - y)**2).sum() \n",
    "        \n",
    "        n = len(ys)  # number of training vectors    \n",
    "        \n",
    "        # get the average change per training input  \n",
    "        # return the average adjustments to the biases and weights \n",
    "        # in each layer and the cost\n",
    "        return del_b2/n, del_w2/n, del_b3/n, del_w3/n, cost/n\n",
    "        \n",
    "    def train(self, epochs, eta):\n",
    "        xs = self.train_inputs\n",
    "        ys = self.train_outputs\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2,d_w2,d_b3,d_w3, cost[e] = self.backprop(xs,ys)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        plt.plot(cost)\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor = XOR_MLP()\n",
    "xs = xor.train_inputs.T\n",
    "\n",
    "print(xor.feedforward(xs))\n",
    "\n",
    "epochs = 1000\n",
    "c = xor.train(epochs, 3.0)\n",
    "\n",
    "print(xor.feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "fig, axs = plt.subplots(3,1,figsize=(10,15))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_axis, c)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x_axis[:61], c[:61])\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x_axis[900:], c[900:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: copy and adapt the above XOR_MLP code so that it uses 3 neurons in the hidden layer. Train such a MLP and see if it learns faster than the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GeneralizedMLP:\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons):\n",
    "        self.input_neurons = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "\n",
    "        # XOR training data\n",
    "        self.train_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "        self.train_outputs = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "        np.random.seed(23)\n",
    "        # Initialize weights and biases\n",
    "        self.w2 = np.random.randn(hidden_neurons, input_neurons)\n",
    "        self.b2 = np.random.randn(hidden_neurons, 1)\n",
    "\n",
    "        self.w3 = np.random.randn(output_neurons, hidden_neurons)\n",
    "        self.b3 = np.random.randn(output_neurons, 1)\n",
    "\n",
    "    def feedforward(self, xs):\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)\n",
    "        return a3s\n",
    "\n",
    "    def backprop(self, xs, ys):\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "\n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            a1 = x.reshape(self.input_neurons, 1)\n",
    "            z2 = self.w2.dot(a1) + self.b2\n",
    "            a2 = sigm(z2)\n",
    "\n",
    "            z3 = self.w3.dot(a2) + self.b3\n",
    "            a3 = sigm(z3)\n",
    "\n",
    "            delta3 = (a3 - y)  # MSE gradient for output layer\n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))\n",
    "\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)\n",
    "\n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)\n",
    "\n",
    "            cost += ((a3 - y)**2).sum()  # Mean Squared Error cost\n",
    "\n",
    "        n = len(ys)\n",
    "        return del_b2 / n, del_w2 / n, del_b3 / n, del_w3 / n, cost / n\n",
    "\n",
    "    def train(self, epochs, eta):\n",
    "        xs = self.train_inputs\n",
    "        ys = self.train_outputs\n",
    "        cost = np.zeros((epochs,))\n",
    "\n",
    "        for e in range(epochs):\n",
    "            d_b2, d_w2, d_b3, d_w3, cost[e] = self.backprop(xs, ys)\n",
    "\n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "\n",
    "        # Plot cost over epochs\n",
    "        plt.plot(cost)\n",
    "        plt.title(\"Training Loss Over Epochs\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Cost (Mean Squared Error)\")\n",
    "        plt.show()\n",
    "\n",
    "        return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the generalized MLP with 3 hidden neurons\n",
    "mlp = GeneralizedMLP(input_neurons=2, hidden_neurons=3, output_neurons=1)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 2000\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Train the model\n",
    "cost = mlp.train(epochs, learning_rate)\n",
    "\n",
    "# Display predictions\n",
    "print(\"Predictions after training:\")\n",
    "for x in mlp.train_inputs:\n",
    "    prediction = mlp.feedforward(x.reshape(2, 1)).flatten()\n",
    "    print(f\"Input: {x}, Predicted: {prediction}\")\n",
    "\n",
    "# Additional tests for cost visualization\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(x_axis, cost)\n",
    "plt.title(\"Cost over all epochs\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(x_axis[:61], cost[:61])\n",
    "plt.title(\"Cost during first 61 epochs\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(x_axis[900:], cost[900:])\n",
    "plt.title(\"Cost during final 100 epochs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MLP class for XOR problem\n",
    "class XOR_MLP:\n",
    "    def __init__(self, hidden_neurons=4):\n",
    "        self.train_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "        self.train_outputs = np.array([0, 1, 1, 0])\n",
    "          \n",
    "        np.random.seed(23)\n",
    "        self.w2 = np.random.randn(hidden_neurons, 2)\n",
    "        self.b2 = np.random.randn(hidden_neurons, 1)\n",
    "        \n",
    "        self.w3 = np.random.randn(hidden_neurons, hidden_neurons)\n",
    "        self.b3 = np.random.randn(hidden_neurons, 1)\n",
    "        \n",
    "        self.w4 = np.random.randn(1, hidden_neurons)\n",
    "        self.b4 = np.random.randn(1, 1)\n",
    "\n",
    "    def feedforward(self, xs):\n",
    "        a2 = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3 = sigm(self.w3.dot(a2) + self.b3)\n",
    "        a4 = sigm(self.w4.dot(a3) + self.b4)\n",
    "        return a4\n",
    "\n",
    "    # Cross-entropy loss function\n",
    "    def cross_entropy_loss(self, a4, y):\n",
    "        return -np.sum(y * np.log(a4))\n",
    "\n",
    "    # Mean squared error loss function\n",
    "    def mse_loss(self, a4, y):\n",
    "        return np.mean((a4 - y) ** 2)\n",
    "\n",
    "    def backprop(self, xs, ys, loss_fn='mse'):\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        \n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        \n",
    "        del_w4 = np.zeros(self.w4.shape, dtype=float)\n",
    "        del_b4 = np.zeros(self.b4.shape, dtype=float)\n",
    "\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x, y in zip(xs, ys):\n",
    "            a1 = x.reshape(2, 1)\n",
    "            z2 = self.w2.dot(a1) + self.b2\n",
    "            a2 = sigm(z2)\n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3\n",
    "            a3 = sigm(z3)\n",
    "            \n",
    "            z4 = self.w4.dot(a3) + self.b4\n",
    "            a4 = sigm(z4)\n",
    "            \n",
    "            if loss_fn == 'cross_entropy':\n",
    "                cost += self.cross_entropy_loss(a4, y)\n",
    "                delta4 = (a4 - y) * sigm_deriv(z4)\n",
    "            elif loss_fn == 'mse':\n",
    "                cost += self.mse_loss(a4, y)\n",
    "                delta4 = 2 * (a4 - y) * sigm_deriv(z4)\n",
    "\n",
    "            delta3 = sigm_deriv(z3) * (self.w4.T.dot(delta4))\n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))\n",
    "\n",
    "            del_b4 += delta4\n",
    "            del_w4 += delta4.dot(a3.T)\n",
    "            \n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)\n",
    "        \n",
    "        n = len(ys)\n",
    "        return del_b2 / n, del_w2 / n, del_b3 / n, del_w3 / n, del_b4 / n, del_w4 / n, cost / n\n",
    "\n",
    "    def train(self, epochs, eta, loss_fn='mse'):\n",
    "        xs = self.train_inputs\n",
    "        ys = self.train_outputs\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2, d_w2, d_b3, d_w3, d_b4, d_w4, cost[e] = self.backprop(xs, ys, loss_fn)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "            self.b4 -= eta * d_b4\n",
    "            self.w4 -= eta * d_w4\n",
    "        \n",
    "        return cost\n",
    "\n",
    "# Instantiate and train the MLP with 4 hidden neurons\n",
    "xor_model = XOR_MLP(hidden_neurons=4)  # 4 neurons as required\n",
    "xs = xor_model.train_inputs.T\n",
    "\n",
    "# Training parameters\n",
    "epochs = 2000  # Training for 2000 iterations\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Train using Cross-Entropy loss\n",
    "cost_cross_entropy = xor_model.train(epochs, learning_rate, loss_fn='cross_entropy')\n",
    "\n",
    "# Train using Mean Squared Error loss\n",
    "cost_mse = xor_model.train(epochs, learning_rate, loss_fn='mse')\n",
    "\n",
    "# Print the output before and after training\n",
    "print(\"Before Training (with Hidden Layers):\")\n",
    "print(xor_model.feedforward(xs))\n",
    "\n",
    "print(\"After Training (with Cross-Entropy Loss):\")\n",
    "print(xor_model.feedforward(xs))\n",
    "\n",
    "# Plotting the cost curves for comparison\n",
    "plt.plot(cost_cross_entropy, label='Cross-Entropy Loss')\n",
    "plt.plot(cost_mse, label='Mean Squared Error Loss')\n",
    "plt.title(\"Cost Over Epochs for Different Loss Functions\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more general purpose MLP with m input neurons, n hidden neurons and o output neurond\n",
    "# You must complete this code yourself\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MLP class\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.uniform(-1, 1, (input_size, hidden_size))\n",
    "        self.bias_hidden = np.zeros((1, hidden_size))\n",
    "        self.weights_hidden_output = np.random.uniform(-1, 1, (hidden_size, output_size))\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = sigm(self.hidden_input)\n",
    "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.final_output = sigm(self.final_input)\n",
    "        return self.final_output\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        # Backward pass using Cross-Entropy loss\n",
    "        output_error = y - output  # Output layer error\n",
    "        output_delta = output_error * sigm_deriv(output)  # Output layer delta\n",
    "\n",
    "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)  # Hidden layer error\n",
    "        hidden_delta = hidden_error * sigm_deriv(self.hidden_output)  # Hidden layer delta\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * self.learning_rate\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "        self.weights_input_hidden += np.dot(X.T, hidden_delta) * self.learning_rate\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "            # Calculate loss using Cross-Entropy\n",
    "            loss = cross_entropy(y, output)\n",
    "            losses.append(loss)\n",
    "            # Backward pass\n",
    "            self.backward(X, y, output)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        return losses\n",
    "\n",
    "# Dataset (XOR problem as an example)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Experiment with different learning rates\n",
    "learning_rates = [0.1, 0.01, 0.001, 1]\n",
    "epochs = 2000\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTraining with learning rate: {lr}\")\n",
    "    mlp = MLP(input_size=2, hidden_size=4, output_size=1, learning_rate=lr)\n",
    "    losses = mlp.train(X, y, epochs)\n",
    "\n",
    "    # Plot loss curve\n",
    "    plt.plot(losses, label=f\"LR={lr}\")\n",
    "\n",
    "# Show loss curves\n",
    "plt.title(\"Loss Curves for Different Learning Rates\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Testing the trained models on a new input\n",
    "test_input = np.array([[1, 0], [0, 1]])  # Example inputs for testing\n",
    "for lr in learning_rates:\n",
    "    mlp = MLP(input_size=2, hidden_size=4, output_size=1, learning_rate=lr)\n",
    "    mlp.train(X, y, epochs)\n",
    "    prediction = mlp.forward(test_input)\n",
    "    print(f\"Prediction for test input with LR={lr}: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the outputs of these correct? They are partially working. I've made correct adjustments below.\n",
    "\"\"\"\n",
    "p1 = MLP(3,4,2)\n",
    "print('\\n W2 = \\n',p1.w2, '\\n W3 = \\n', p1.w3, '\\n')\n",
    "\n",
    "p2 = MLP(4,6,3)\n",
    "print('\\n W2 = \\n', p2.w2, '\\nW3 = \\n', p2.w3, '\\n')\n",
    "\"\"\"\n",
    "\n",
    "# Corrected MLP instances\n",
    "p1 = MLP(3, 4, 2, learning_rate=0.1)  # Added learning_rate\n",
    "print('\\nWeights between Input and Hidden Layer (W2): \\n', p1.weights_input_hidden)\n",
    "print('\\nWeights between Hidden and Output Layer (W3): \\n', p1.weights_hidden_output)\n",
    "\n",
    "p2 = MLP(4, 6, 3, learning_rate=1)  # Added learning_rate\n",
    "print('\\nWeights between Input and Hidden Layer (W2): \\n', p2.weights_input_hidden)\n",
    "print('\\nWeights between Hidden and Output Layer (W3): \\n', p2.weights_hidden_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MLP Class\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.uniform(-1, 1, (input_size, hidden_size))\n",
    "        self.bias_hidden = np.zeros((1, hidden_size))\n",
    "        self.weights_hidden_output = np.random.uniform(-1, 1, (hidden_size, output_size))\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = sigm(self.hidden_input)\n",
    "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.final_output = sigm(self.final_input)\n",
    "        return self.final_output\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        # Backpropagation\n",
    "        output_error = y - output  # Output layer error\n",
    "        output_delta = output_error * sigm_deriv(output)  # Output delta\n",
    "        \n",
    "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)  # Hidden layer error\n",
    "        hidden_delta = hidden_error * sigm_deriv(self.hidden_output)  # Hidden delta\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * self.learning_rate\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "        self.weights_input_hidden += np.dot(X.T, hidden_delta) * self.learning_rate\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs, cost_function):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            loss = cost_function(y, output)\n",
    "            losses.append(loss)\n",
    "            self.backward(X, y, output)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        return losses\n",
    "\n",
    "# Training Data\n",
    "X_training = np.array([\n",
    "    [1, 1, 0],\n",
    "    [1, -1, -1],\n",
    "    [-1, 1, 1],\n",
    "    [-1, -1, 1],\n",
    "    [0, 1, -1],\n",
    "    [0, -1, -1],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "y_training = np.array([\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "# Parameters\n",
    "input_size = 3\n",
    "output_size = 2\n",
    "hidden_sizes = [4, 8]  # Try different hidden layer sizes\n",
    "learning_rates = [0.1, 0.01]  # Experiment with different learning rates\n",
    "epochs = 2000\n",
    "\n",
    "# Experiment with different hidden sizes and learning rates\n",
    "for hidden_size in hidden_sizes:\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nTraining with hidden_size={hidden_size}, learning_rate={lr}\")\n",
    "        mlp = MLP(input_size, hidden_size, output_size, learning_rate=lr)\n",
    "        losses = mlp.train(X_training, y_training, epochs, cross_entropy)\n",
    "        \n",
    "        # Plot the loss curve\n",
    "        plt.plot(losses, label=f\"Hidden={hidden_size}, LR={lr}\")\n",
    "\n",
    "# Plot settings\n",
    "plt.title(\"Loss Curves for Different Hidden Sizes and Learning Rates with Cross-Entropy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MLP Class\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.uniform(-1, 1, (input_size, hidden_size))\n",
    "        self.bias_hidden = np.zeros((1, hidden_size))\n",
    "        self.weights_hidden_output = np.random.uniform(-1, 1, (hidden_size, output_size))\n",
    "        self.bias_output = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = sigm(self.hidden_input)\n",
    "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.final_output = sigm(self.final_input)\n",
    "        return self.final_output\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        # Backpropagation\n",
    "        output_error = y - output  # Output layer error\n",
    "        output_delta = output_error * sigm_deriv(output)  # Output delta\n",
    "        \n",
    "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)  # Hidden layer error\n",
    "        hidden_delta = hidden_error * sigm_deriv(self.hidden_output)  # Hidden delta\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * self.learning_rate\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "        self.weights_input_hidden += np.dot(X.T, hidden_delta) * self.learning_rate\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate\n",
    "\n",
    "    def train(self, X, y, epochs, cost_function):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            loss = cost_function(y, output)\n",
    "            losses.append(loss)\n",
    "            self.backward(X, y, output)\n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        return losses\n",
    "\n",
    "# Training Data\n",
    "data = np.array([\n",
    "    [0, 1, 0, 0],  # Male, 1 car, Low travel cost, Low income\n",
    "    [1, 0, 1, 1],  # Female, No car, Medium travel cost, Medium income\n",
    "    [0, 2, 2, 2],  # Male, 2 cars, High travel cost, High income\n",
    "    [1, 0, 2, 1],  # Female, No car, High travel cost, Medium income\n",
    "    [0, 1, 1, 0],  # Male, 1 car, Medium travel cost, Low income\n",
    "    [1, 0, 0, 2],  # Female, No car, Low travel cost, High income\n",
    "    [1, 1, 1, 1],  # Female, 1 car, Medium travel cost, Medium income\n",
    "    [0, 0, 2, 0],  # Male, No car, High travel cost, Low income\n",
    "    [1, 2, 1, 2],  # Female, 2 cars, Medium travel cost, High income\n",
    "    [0, 0, 0, 1]   # Male, No car, Low travel cost, Medium income\n",
    "])\n",
    "\n",
    "targets = np.array([\n",
    "    [1, 0, 0],  # Bus\n",
    "    [0, 1, 0],  # Car\n",
    "    [0, 0, 1],  # Train\n",
    "    [0, 0, 1],  # Train\n",
    "    [1, 0, 0],  # Bus\n",
    "    [0, 1, 0],  # Car\n",
    "    [0, 1, 0],  # Car\n",
    "    [1, 0, 0],  # Bus\n",
    "    [0, 0, 1],  # Train\n",
    "    [1, 0, 0]   # Bus\n",
    "])\n",
    "\n",
    "# Save to CSV file using Pandas\n",
    "df = pd.DataFrame(data, columns=['Gender', 'Car Ownership', 'Travel Cost', 'Income Level'])\n",
    "df['Target'] = [tuple(t) for t in targets]\n",
    "df.to_csv('transport.csv', index=False)\n",
    "print(\"Training data saved to transport.csv\")\n",
    "\n",
    "# Experiment with different hyperparameters\n",
    "input_size = 4\n",
    "output_size = 3\n",
    "hidden_sizes = [6, 8]  # Experiment with different hidden layer sizes\n",
    "learning_rates = [0.1, 0.01]  # Experiment with different learning rates\n",
    "epochs = 2000\n",
    "\n",
    "# Experiment with different hidden sizes and learning rates\n",
    "for hidden_size in hidden_sizes:\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nTraining with hidden_size={hidden_size}, learning_rate={lr}\")\n",
    "        mlp = MLP(input_size, hidden_size, output_size, learning_rate=lr)\n",
    "        losses = mlp.train(data, targets, epochs, cross_entropy)\n",
    "        \n",
    "        # Plot the loss curve\n",
    "        plt.plot(losses, label=f\"Hidden={hidden_size}, LR={lr}\")\n",
    "\n",
    "# Plot settings\n",
    "plt.title(\"Loss Curves for Different Hidden Sizes and Learning Rates with Cross-Entropy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Prediction\n",
    "test_instance = np.array([[1, 0, 2, 1]])  # Female, No car, High travel cost, Medium income\n",
    "prediction = mlp.forward(test_instance)\n",
    "predicted_class = np.argmax(prediction)  # Find the class with the highest probability\n",
    "\n",
    "print(f\"Predicted output: {prediction}\")\n",
    "print(f\"Predicted transportation mode: {['Bus', 'Car', 'Train'][predicted_class]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "iris_data = pd.read_csv('iris_data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = iris_data.iloc[:, :-1].values\n",
    "y = iris_data.iloc[:, -1].values\n",
    "\n",
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_one_hot = to_categorical(y_encoded)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Experimenting with different hidden sizes and learning rates\n",
    "hidden_sizes = [8, 10, 12]  # Experiment with different hidden layer sizes\n",
    "learning_rates = [0.01, 0.001]  # Experiment with different learning rates\n",
    "epochs = 100\n",
    "\n",
    "# Initialize a dictionary to hold loss curves for each configuration\n",
    "losses = []\n",
    "\n",
    "# Create and train models for different hyperparameters\n",
    "for hidden_size in hidden_sizes:\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nTraining with hidden_size={hidden_size}, learning_rate={lr}\")\n",
    "        \n",
    "        # Create model\n",
    "        model = Sequential([\n",
    "            Input(shape=(4,)),  # Define input shape explicitly\n",
    "            Dense(hidden_size, activation='relu'),\n",
    "            Dense(hidden_size, activation='relu'),\n",
    "            Dense(y_one_hot.shape[1], activation='softmax')\n",
    "        ])\n",
    "\n",
    "        # Compile model with custom learning rate\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0)\n",
    "        \n",
    "        # Record the loss history\n",
    "        losses.append((hidden_size, lr, history.history['loss'], history.history['val_loss']))\n",
    "\n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Plot Training and Validation Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'Training and Validation Loss (Hidden={hidden_size}, LR={lr})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'Training and Validation Accuracy (Hidden={hidden_size}, LR={lr})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Evaluate with the best model configuration\n",
    "best_model = model  # You can select the model with the best performance here\n",
    "\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Prediction function\n",
    "def predict_new_sample(sample):\n",
    "    prediction = best_model.predict(sample)\n",
    "    return le.inverse_transform([np.argmax(prediction)])\n",
    "\n",
    "# Example usage: Predict for a new sample\n",
    "new_sample = np.array([[5.1, 3.5, 1.4, 0.2]])  # Example sample, replace with actual input\n",
    "scaled_sample = scaler.transform(new_sample)\n",
    "print(\"Prediction for new sample:\", predict_new_sample(scaled_sample))\n",
    "\n",
    "# Print class labels\n",
    "print(\"\\nClass Labels:\", le.classes_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
